{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VOBHP7oTqmx"
      },
      "source": [
        "# useful packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCi2bz6zToJM",
        "outputId": "9da5e3d0-0b05-4fa7-d8fb-cd6ce8e299a0"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy\n",
        "!pip install stable-baselines3 gym\n",
        "!pip install torch torchvision\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqEwVTR0TqNJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import pickle\n",
        "from collections import defaultdict, deque\n",
        "import pandas as pd\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kl9x7kE98Ft",
        "outputId": "1337e042-8767-464f-bc54-c08133d04f45"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpfmukmDTct1"
      },
      "source": [
        "# logics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qu5mNP9TVUb"
      },
      "outputs": [],
      "source": [
        "class LiarDiceGame:\n",
        "    def __init__(self):\n",
        "        self.dice_count = {1: 5, 2: 5}\n",
        "        self.players = {1: [random.randint(1, 6) for _ in range(self.dice_count[1])],\n",
        "                        2: [random.randint(1, 6) for _ in range(self.dice_count[2])]}\n",
        "        self.current_bid = (1, 1)  # Minimum bid to start each round\n",
        "        self.current_player = 1\n",
        "        self.last_action_was_challenge = False\n",
        "        self.scores = {1: 0, 2: 0}\n",
        "        self.player_names = {1: 'Player 1', 2: 'Player 2'} # default player namess\n",
        "\n",
        "    def set_player_names(self, player1_name, player2_name):\n",
        "        self.player_names[1] = player1_name\n",
        "        self.player_names[2] = player2_name\n",
        "\n",
        "    def roll_dice(self):\n",
        "        for player in self.players:\n",
        "            self.players[player] = [random.randint(1, 6) for _ in range(self.dice_count[player])]\n",
        "\n",
        "    def reveal_dice(self):\n",
        "        return self.players\n",
        "\n",
        "    def make_bid(self, player, quantity, face_value):\n",
        "        if face_value not in range(1, 7):\n",
        "            return False\n",
        "\n",
        "        if quantity > 10 or quantity < 1:\n",
        "            return False\n",
        "\n",
        "        if self.current_bid == (1, 1):\n",
        "            if quantity < 1 or face_value < 1:\n",
        "                return False\n",
        "        else:\n",
        "            if quantity < self.current_bid[0] or (quantity == self.current_bid[0] and face_value <= self.current_bid[1]):\n",
        "                return False\n",
        "\n",
        "        self.current_bid = (quantity, face_value)\n",
        "        self.last_action_was_challenge = False\n",
        "        self.switch_player()\n",
        "        return True\n",
        "\n",
        "    def adjust_scores(self, winner, loser):\n",
        "        self.scores[winner] += 100\n",
        "        self.scores[loser] = max(self.scores[loser] - 100, 0)\n",
        "\n",
        "\n",
        "    def challenge(self, challenger):\n",
        "        players_dice = self.reveal_dice()\n",
        "        if self.current_bid[1] == 1:\n",
        "            total_quantity = sum(dice.count(1) for dice in players_dice.values())\n",
        "        else:\n",
        "            total_quantity = sum(dice.count(self.current_bid[1]) + dice.count(1) for dice in players_dice.values())\n",
        "\n",
        "        result = None\n",
        "        dice_faces = {player: \" \".join(str(die) for die in dice) for player, dice in players_dice.items()}\n",
        "\n",
        "        if total_quantity >= self.current_bid[0]:\n",
        "            self.switch_player()\n",
        "            result = f\"Challenge failed. Total dice count is {total_quantity}. {self.player_names[challenger]} loses a dice and 100 points. {self.player_names[self.current_player]} wins 100 points.\\n\" \\\n",
        "                     f\"{self.player_names[1]}'s dice: {dice_faces[1]}\\n{self.player_names[2]}'s dice: {dice_faces[2]}\"\n",
        "            self.dice_count[challenger] -= 1\n",
        "            if self.dice_count[challenger] > 0:\n",
        "                self.players[challenger].pop()\n",
        "            self.adjust_scores(self.current_player, challenger)\n",
        "        else:\n",
        "            self.switch_player()\n",
        "            result = f\"Challenge successful. Total dice count is {total_quantity}. {self.player_names[self.current_player]} loses a dice and 100 points. {self.player_names[challenger]} wins 100 points.\\n\" \\\n",
        "                     f\"{self.player_names[1]}'s dice: {dice_faces[1]}\\n{self.player_names[2]}'s dice: {dice_faces[2]}\"\n",
        "            self.dice_count[self.current_player] -= 1\n",
        "            if self.dice_count[self.current_player] > 0:\n",
        "                self.players[self.current_player].pop()\n",
        "            self.adjust_scores(challenger, self.current_player)\n",
        "\n",
        "        self.last_action_was_challenge = True\n",
        "        self.current_bid = (0, 0)\n",
        "        self.roll_dice()\n",
        "\n",
        "        if self.is_game_over():\n",
        "            return result\n",
        "\n",
        "        return result\n",
        "\n",
        "    def switch_player(self):\n",
        "        self.current_player = 1 if self.current_player == 2 else 2\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return any(count == 0 for count in self.dice_count.values())\n",
        "\n",
        "    def get_winner(self):\n",
        "        if self.dice_count[1] == 0:\n",
        "            return 2\n",
        "        elif self.dice_count[2] == 0:\n",
        "            return 1\n",
        "        return None\n",
        "\n",
        "    def random_bid(self):\n",
        "        total_dice = 10\n",
        "\n",
        "        min_quantity = self.current_bid[0] + 1\n",
        "        if min_quantity > total_dice:\n",
        "            min_quantity = total_dice\n",
        "\n",
        "        quantity = random.randint(min_quantity, total_dice)\n",
        "        face_value = random.randint(1, 6)\n",
        "        return quantity, face_value\n",
        "\n",
        "    def get_dice_counts(self):\n",
        "        return list(self.dice_count.values())\n",
        "\n",
        "    def get_game_state(self):\n",
        "        return {\n",
        "            \"dice_count\": self.get_dice_counts(),  # Use the method to get a list\n",
        "            \"players\": self.players,\n",
        "            \"current_bid\": self.current_bid,\n",
        "            \"current_player\": self.current_player,\n",
        "            \"last_action_was_challenge\": self.last_action_was_challenge,\n",
        "            \"player_names\": self.player_names,\n",
        "            \"scores\": self.scores,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5e45ouDTTU3"
      },
      "source": [
        "# env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSMJtQtcTEqn"
      },
      "outputs": [],
      "source": [
        "class LiarDiceEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(LiarDiceEnv, self).__init__()\n",
        "        self.game = LiarDiceGame()\n",
        "\n",
        "        self.action_space = spaces.Tuple((\n",
        "            spaces.Discrete(2),  # 0: make_bid, 1: challenge\n",
        "            spaces.Discrete(11),  # quantity: 1-10\n",
        "            spaces.Discrete(6)   # face_value: 1-6\n",
        "        ))\n",
        "\n",
        "        self.observation_space = spaces.Dict({\n",
        "            \"dice_count\": spaces.Box(low=0, high=5, shape=(2,), dtype=np.int32),\n",
        "            \"current_bid\": spaces.Box(low=0, high=10, shape=(2,), dtype=np.int32),\n",
        "            \"scores\": spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.int32),\n",
        "            \"current_player\": spaces.Discrete(2),\n",
        "            \"players_dice\": spaces.Box(low=0, high=6, shape=(2, 5), dtype=np.int32)  # Padded players' dice values\n",
        "        })\n",
        "\n",
        "    def reset(self):\n",
        "        self.game = LiarDiceGame()\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        state = self.game.get_game_state()\n",
        "        players_dice_padded = [self.pad_dice(self.game.players[1]), self.pad_dice(self.game.players[2])]\n",
        "        return {\n",
        "            \"dice_count\": np.array(state['dice_count']),\n",
        "            \"current_bid\": np.array(state['current_bid']),\n",
        "            \"scores\": np.array([state['scores'][1], state['scores'][2]]),\n",
        "            \"current_player\": state['current_player'] - 1,  # adjust to 0-indexed\n",
        "            \"players_dice\": np.array(players_dice_padded)  # Padded players' dice values\n",
        "        }\n",
        "\n",
        "    def pad_dice(self, dice, max_length=5):\n",
        "        return dice + [0] * (max_length - len(dice))\n",
        "\n",
        "    def step(self, action):\n",
        "        if isinstance(action, int):\n",
        "            action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "        else:\n",
        "            action_type, quantity, face_value = action\n",
        "        if action_type == 0:  # make_bid\n",
        "            success = self.game.make_bid(self.game.current_player, quantity, face_value)\n",
        "            if not success:\n",
        "                reward = -10  # Penalize invalid bids\n",
        "                done = True\n",
        "                return self._get_obs(), reward, done, {}\n",
        "            else:\n",
        "                reward = 1  # Small reward for valid bids\n",
        "        elif action_type == 1:  # challenge\n",
        "            result = self.game.challenge(self.game.current_player)\n",
        "            if \"successful\" in result:\n",
        "                reward = 100\n",
        "            else:\n",
        "                reward = -100\n",
        "\n",
        "        done = self.game.is_game_over()\n",
        "        if done:\n",
        "            if self.game.get_winner() == self.game.current_player:\n",
        "                reward += 100  # Large reward for winning\n",
        "            else:\n",
        "                reward -= 100  # Large penalty for losing\n",
        "\n",
        "        return self._get_obs(), reward, done, {}\n",
        "\n",
        "    def random_bid(self):\n",
        "        total_dice = 10\n",
        "        min_quantity = self.game.current_bid[0] + 1\n",
        "        if min_quantity > total_dice:\n",
        "            min_quantity = total_dice\n",
        "        quantity = random.randint(min_quantity, total_dice)\n",
        "        face_value = random.randint(1, 6)\n",
        "        return (0, quantity, face_value)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        state = self.game.get_game_state()\n",
        "        print(state)\n",
        "\n",
        "    def close(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg9cJRoWUAUj"
      },
      "source": [
        "# models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbhQmEHvUBkC"
      },
      "source": [
        "## Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-tKrLM-UBF0"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, state_size, action_size, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.q_table = defaultdict(lambda: np.zeros(action_size))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        # Flatten state dictionary into a tuple\n",
        "        return tuple(np.concatenate([\n",
        "            state[\"dice_count\"],\n",
        "            state[\"current_bid\"],\n",
        "            state[\"scores\"],\n",
        "            [state[\"current_player\"]],\n",
        "            state[\"players_dice\"].flatten()\n",
        "        ]))\n",
        "\n",
        "    def get_action_space(self):\n",
        "        actions = [(0, quantity, face_value) for quantity in range(1, 11) for face_value in range(1, 7)]\n",
        "        actions.append((1, 0, 0))  # Challenge action\n",
        "        return actions\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.alpha * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def reward_shaping(self, reward, action, done):\n",
        "        if action // 60 == 1:  # Challenge action\n",
        "            if reward > 0:\n",
        "                reward += 50  # Additional reward for a successful challenge\n",
        "            else:\n",
        "                reward -= 50  # Additional penalty for a failed challenge\n",
        "        if done:\n",
        "            if reward > 0:\n",
        "                reward += 100  # Additional reward for winning\n",
        "            else:\n",
        "                reward -= 100  # Additional penalty for losing\n",
        "        return reward\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(dict(self.q_table), f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.q_table = defaultdict(lambda: np.zeros(self.action_size), pickle.load(f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc8tS9gnUIKB"
      },
      "source": [
        "## Deep Q Network - DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NMbe43UTUM7d"
      },
      "outputs": [],
      "source": [
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=64, memory_size=10000):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = DQNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_model = DQNetwork(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        act_values = self.model(state)\n",
        "        return torch.argmax(act_values, dim=1).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
        "            reward = torch.tensor(reward, device=self.device)\n",
        "            done = torch.tensor(done, device=self.device)\n",
        "\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * torch.max(self.target_model(next_state)).item()\n",
        "            target_f = self.model(state)\n",
        "            target_f[0][action] = target\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = F.mse_loss(target_f, self.model(state))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'state_size': self.state_size,\n",
        "                'action_size': self.action_size,\n",
        "                'gamma': self.gamma,\n",
        "                'epsilon': self.epsilon,\n",
        "                'epsilon_decay': self.epsilon_decay,\n",
        "                'epsilon_min': self.epsilon_min,\n",
        "                'batch_size': self.batch_size,\n",
        "                'memory': list(self.memory),\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            }, f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "            self.state_size = checkpoint['state_size']\n",
        "            self.action_size = checkpoint['action_size']\n",
        "            self.gamma = checkpoint['gamma']\n",
        "            self.epsilon = checkpoint['epsilon']\n",
        "            self.epsilon_decay = checkpoint['epsilon_decay']\n",
        "            self.epsilon_min = checkpoint['epsilon_min']\n",
        "            self.batch_size = checkpoint['batch_size']\n",
        "            self.memory = deque(checkpoint['memory'], maxlen=10000)\n",
        "            self.model = DQNetwork(self.state_size, self.action_size).to(self.device)\n",
        "            self.target_model = DQNetwork(self.state_size, self.action_size).to(self.device)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer = optim.Adam(self.model.parameters())\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.update_target_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVXEa2vzUNOV"
      },
      "source": [
        "## Bayesian model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "bOcSSGbMUPnD"
      },
      "outputs": [],
      "source": [
        "class BayesianAgent:\n",
        "    def __init__(self, num_dice, num_faces=6, bluff_probability=0.1):\n",
        "        self.num_dice = num_dice\n",
        "        self.num_faces = num_faces\n",
        "        self.bluff_probability = bluff_probability\n",
        "        self.opponent_bids = []\n",
        "\n",
        "    def update_beliefs(self, observed_dice):\n",
        "        counts = np.zeros(self.num_faces + 1)\n",
        "        for die in observed_dice:\n",
        "            counts[die] += 1\n",
        "        return counts\n",
        "\n",
        "    def make_bid(self, current_bid, observed_dice):\n",
        "        counts = self.update_beliefs(observed_dice)\n",
        "        estimated_opponent_counts = self.estimate_opponent_dice()\n",
        "        total_estimated_counts = counts + estimated_opponent_counts\n",
        "\n",
        "        possible_actions = [(q, f) for q in range(current_bid[0] + 1, min(self.num_dice * 2 + 1, 10 + 1)) for f in range(1, self.num_faces + 1)]\n",
        "\n",
        "        if not possible_actions:\n",
        "            print(\"Error: No possible actions available.\")\n",
        "            possible_actions = [(current_bid[0] + 1, 1)]\n",
        "\n",
        "        if self.should_bluff():\n",
        "            best_action = self.make_bluff_bid(current_bid)\n",
        "        else:\n",
        "            best_action = None\n",
        "            max_expected_value = -np.inf\n",
        "            for action in possible_actions:\n",
        "                expected_value = self.calculate_expected_value(action, total_estimated_counts)\n",
        "                if expected_value > max_expected_value:\n",
        "                    max_expected_value = expected_value\n",
        "                    best_action = action\n",
        "\n",
        "            if best_action is None:\n",
        "                best_action = random.choice(possible_actions)\n",
        "\n",
        "        print(f\"Making bid: Current bid {current_bid}, New bid {best_action}, Observed counts {counts}, Estimated opponent counts {estimated_opponent_counts}\")\n",
        "        return best_action\n",
        "\n",
        "    def calculate_expected_value(self, action, counts):\n",
        "        quantity, face_value = action\n",
        "        total_count = sum(counts)\n",
        "        if total_count == 0:\n",
        "            return 0\n",
        "        probability = counts[face_value] / total_count\n",
        "        return quantity * probability\n",
        "\n",
        "    def should_bluff(self):\n",
        "        return np.random.rand() < self.bluff_probability\n",
        "\n",
        "    def make_bluff_bid(self, current_bid):\n",
        "        return (min(current_bid[0] + 2, 10), (current_bid[1] % self.num_faces) + 1)\n",
        "\n",
        "    def should_challenge(self, current_bid, observed_dice):\n",
        "        counts = self.update_beliefs(observed_dice)\n",
        "        total_count = sum(counts[1:]) + counts[1]\n",
        "\n",
        "        self.update_opponent_bids(current_bid)\n",
        "        estimated_opponent_counts = self.estimate_opponent_dice()\n",
        "        total_estimated_counts = counts + estimated_opponent_counts\n",
        "        total_count_with_estimates = sum(total_estimated_counts[1:]) + total_estimated_counts[1]\n",
        "\n",
        "        challenge = total_count_with_estimates < current_bid[0]\n",
        "        print(f\"Challenge decision: Current bid {current_bid}, Observed counts {counts}, Estimated opponent counts {estimated_opponent_counts}, Total count with estimates {total_count_with_estimates}, Challenge {challenge}\")\n",
        "        return challenge\n",
        "\n",
        "    def update_opponent_bids(self, bid):\n",
        "        self.opponent_bids.append(bid)\n",
        "\n",
        "    def estimate_opponent_dice(self):\n",
        "        if not self.opponent_bids:\n",
        "            return np.zeros(self.num_faces + 1)\n",
        "        estimated_counts = np.zeros(self.num_faces + 1)\n",
        "        for bid in self.opponent_bids:\n",
        "            estimated_counts[bid[1]] += bid[0] / len(self.opponent_bids)\n",
        "        return estimated_counts\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'num_dice': self.num_dice,\n",
        "                'num_faces': self.num_faces,\n",
        "                'opponent_bids': self.opponent_bids,\n",
        "            }, f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "            self.num_dice = checkpoint['num_dice']\n",
        "            self.num_faces = checkpoint['num_faces']\n",
        "            self.opponent_bids = checkpoint['opponent_bids']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh211FSkqIK1"
      },
      "source": [
        "## SARSA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ08O3a7qHG9"
      },
      "outputs": [],
      "source": [
        "class SARSAAgent:\n",
        "    def __init__(self, state_size, action_size, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.q_table = defaultdict(lambda: np.zeros(action_size))\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        q_values = self.q_table[state]\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
        "        td_target = reward + self.gamma * self.q_table[next_state][next_action]\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.alpha * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(dict(self.q_table), f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.q_table = defaultdict(lambda: np.zeros(self.action_size), pickle.load(f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAj2k1jJvPuj"
      },
      "source": [
        "## MCTS model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "zxSqLlBCvPcc"
      },
      "outputs": [],
      "source": [
        "class MCTSAgent:\n",
        "    def __init__(self, num_simulations=500):\n",
        "        self.num_simulations = num_simulations\n",
        "\n",
        "    def select_action(self, state, env):\n",
        "        root = Node(state, None, None, env)\n",
        "        for _ in range(self.num_simulations):\n",
        "            leaf = self.traverse(root, env)\n",
        "            reward = self.rollout(leaf.state, env)\n",
        "            self.backpropagate(leaf, reward)\n",
        "        return self.best_action(root)\n",
        "\n",
        "    def traverse(self, node, env):\n",
        "        while not node.is_terminal():\n",
        "            if node.is_fully_expanded():\n",
        "                node = self.best_child(node)\n",
        "            else:\n",
        "                return self.expand(node, env)\n",
        "        return node\n",
        "\n",
        "    def expand(self, node, env):\n",
        "        action = node.untried_actions.pop()\n",
        "        action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "        next_state, _, done, _ = env.step((action_type, quantity, face_value))\n",
        "        child_node = Node(next_state, node, action, env)\n",
        "        node.children.append(child_node)\n",
        "        return child_node\n",
        "\n",
        "    def rollout(self, state, env):\n",
        "        current_state = state\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0  # Limit the depth of the rollout to avoid long rollouts\n",
        "\n",
        "        while not done and steps < 50:  # Limit rollout depth to 50 steps\n",
        "            action = self.rollout_policy(env, current_state)\n",
        "            action_type, quantity, face_value = action\n",
        "            next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "            current_state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def rollout_policy(self, env, state):\n",
        "        # A simple heuristic policy for rollouts: choose the action with the highest quantity and face value\n",
        "        action_space = [(0, quantity, face_value) for quantity in range(10) for face_value in range(1, 7)]\n",
        "        return random.choice(action_space)  # This can be replaced with a more sophisticated policy\n",
        "\n",
        "    def backpropagate(self, node, reward):\n",
        "        while node is not None:\n",
        "            node.visits += 1\n",
        "            node.reward += reward\n",
        "            node = node.parent\n",
        "\n",
        "    def best_action(self, node):\n",
        "        best_child = max(node.children, key=lambda child: child.reward / child.visits)\n",
        "        return best_child.action\n",
        "\n",
        "    def best_child(self, node):\n",
        "        # Use UCT (Upper Confidence Bound for Trees) for selection\n",
        "        C = 1.41  # Exploration-exploitation balance constant\n",
        "        return max(node.children, key=lambda child: child.reward / child.visits + C * np.sqrt(np.log(node.visits) / child.visits))\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent, action, env):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.reward = 0\n",
        "        self.untried_actions = list(range(2 * 10 * 6))  # 2 action types, 10 quantities, 6 face values\n",
        "        self.env = env\n",
        "        self.action = action\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.env.game.is_game_over()\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.untried_actions) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt1wDmWIUQGB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94w987MwURpY"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wx_vreEoURSe",
        "outputId": "5bda8fa6-54ce-4d4c-c21b-3a1b8b022e44"
      },
      "outputs": [],
      "source": [
        "def train_q_learning_agent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = agent.preprocess_state(state)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        step = 0\n",
        "        while not done:\n",
        "            action_index = agent.get_action(state)\n",
        "            action = agent.get_action_space()[action_index]\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = agent.preprocess_state(next_state)\n",
        "            shaped_reward = agent.reward_shaping(reward, action_index, done)\n",
        "            agent.update_q_table(state, action_index, shaped_reward, next_state)\n",
        "            state = next_state\n",
        "            episode_reward += shaped_reward\n",
        "            step += 1\n",
        "\n",
        "            # Detailed logging for diagnosis\n",
        "            if e % 1000 == 0:\n",
        "                print(f\"Episode: {e+1}, Step: {step}, Action: {action}, Reward: {reward}, Shaped Reward: {shaped_reward}, Next State: {next_state}\")\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        rewards.append(episode_reward)\n",
        "        if e % 50 == 0:\n",
        "            agent.save(f'q_learning_agent_{e}.pkl')\n",
        "        print(f\"Episode: {e+1}/{episodes}, Steps: {step}, Reward: {episode_reward}, Epsilon: {agent.epsilon}\")\n",
        "    agent.save('q_learning_agent_final.pkl')\n",
        "    return rewards\n",
        "\n",
        "# Initializing the environment and agent\n",
        "env = LiarDiceEnv()\n",
        "state_size = 17  # dice_count(2) + current_bid(2) + scores(2) + current_player(1) + players_dice(10)\n",
        "action_size = len([(0, quantity, face_value) for quantity in range(1, 11) for face_value in range(1, 7)]) + 1  # 2 types, 10 quantities, 6 face values\n",
        "q_learning_agent = QLearningAgent(state_size, action_size)\n",
        "\n",
        "# Training the agent\n",
        "q_learning_rewards = train_q_learning_agent(env, q_learning_agent, episodes=10000)\n",
        "\n",
        "# Plotting rewards\n",
        "plt.plot(q_learning_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Q-Learning Agent Training Rewards')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAPT__mUTol"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cbH23_bNUVFR",
        "outputId": "d8a82829-d6f1-478a-ed61-940c31c5c3b0"
      },
      "outputs": [],
      "source": [
        "def train_dqn_agent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.concatenate([state[\"dice_count\"], state[\"current_bid\"], state[\"scores\"], [state[\"current_player\"]], state[\"players_dice\"].flatten()])\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "            next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "            next_state = np.concatenate([next_state[\"dice_count\"], next_state[\"current_bid\"], next_state[\"scores\"], [next_state[\"current_player\"]], next_state[\"players_dice\"].flatten()])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            if done:\n",
        "                agent.update_target_model()\n",
        "                print(f\"Episode: {e+1}/{episodes}, Reward: {episode_reward}, Epsilon: {agent.epsilon}\")\n",
        "        agent.replay()\n",
        "        rewards.append(episode_reward)\n",
        "        if e % 50 == 0:\n",
        "            agent.save(f'dqn_agent_{e}.pkl')\n",
        "    agent.save('dqn_agent_final.pkl')\n",
        "    return rewards\n",
        "\n",
        "# Initializing the environment and agent\n",
        "env = LiarDiceEnv()\n",
        "state_size = 17  # dice_count(2) + current_bid(2) + scores(2) + current_player(1) + players_dice(10)\n",
        "action_size = 2 * 10 * 6  # 2 types, 10 quantities, 6 face values\n",
        "dqn_agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Training the agent\n",
        "dqn_rewards = train_dqn_agent(env, dqn_agent, episodes=10000)\n",
        "\n",
        "# Plotting rewards\n",
        "plt.plot(dqn_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('DQN Agent Training Rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYn8er8KUVO4"
      },
      "source": [
        "## Bayesian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IYPl3dVAUaT6",
        "outputId": "fce10118-5e08-4bf3-dd26-56fcf7257b4c"
      },
      "outputs": [],
      "source": [
        "def train_bayesian_agent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        step = 0\n",
        "\n",
        "        while not done:\n",
        "            current_player = state[\"current_player\"]\n",
        "            opponent = 1 if current_player == 0 else 0\n",
        "            observed_dice = state[\"players_dice\"][opponent]\n",
        "            current_bid = state[\"current_bid\"]\n",
        "\n",
        "            if current_player == 0:\n",
        "                if agent.should_challenge(current_bid, observed_dice):\n",
        "                    action = (1, 0, 0)  # Challenge\n",
        "                else:\n",
        "                    quantity, face_value = agent.make_bid(current_bid, observed_dice)\n",
        "                    action = (0, quantity, face_value)\n",
        "            else:\n",
        "                action = env.random_bid()\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update_opponent_bids(current_bid)\n",
        "\n",
        "            if current_player == 0:\n",
        "                observed_dice = next_state[\"players_dice\"][opponent]\n",
        "                agent.update_beliefs(observed_dice)\n",
        "                episode_reward += reward\n",
        "\n",
        "            state = next_state\n",
        "            step += 1\n",
        "\n",
        "            if e % 1000 == 0:\n",
        "                print(f\"Episode: {e+1}, Step: {step}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        if e % 50 == 0:\n",
        "            agent.save(f'bayesian_agent_{e}.pkl')\n",
        "        print(f\"Episode: {e+1}/{episodes}, Steps: {step}, Reward: {episode_reward}\")\n",
        "\n",
        "    agent.save('bayesian_agent_final.pkl')\n",
        "    return rewards\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = LiarDiceEnv()\n",
        "bayesian_agent = BayesianAgent(num_dice=5)\n",
        "\n",
        "# Train the agent\n",
        "bayesian_rewards = train_bayesian_agent(env, bayesian_agent, episodes=10000)\n",
        "\n",
        "# Plot rewards\n",
        "plt.plot(bayesian_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Bayesian Agent Training Rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFSn2eSywCbM"
      },
      "source": [
        "## SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QxpcuG3WwBVb",
        "outputId": "7cafb534-e4f5-4e78-a43d-dbfe7fbb5cf5"
      },
      "outputs": [],
      "source": [
        "def train_sarsa_agent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = tuple(state[\"dice_count\"]) + tuple(state[\"current_bid\"]) + tuple(state[\"scores\"]) + (state[\"current_player\"],)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        action = agent.get_action(state)\n",
        "\n",
        "        while not done:\n",
        "            action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "            next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "            next_state = tuple(next_state[\"dice_count\"]) + tuple(next_state[\"current_bid\"]) + tuple(next_state[\"scores\"]) + (next_state[\"current_player\"],)\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    reward = 100  # Winning the game\n",
        "                elif reward < 0:\n",
        "                    reward = -100  # Losing the game\n",
        "\n",
        "            next_action = agent.get_action(next_state)\n",
        "            agent.update_q_table(state, action, reward, next_state, next_action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        rewards.append(episode_reward)\n",
        "        if e % 50 == 0:\n",
        "            agent.save(f'simple_sarsa_agent_{e}.pkl')\n",
        "        print(f\"Episode: {e+1}/{episodes}, Total Reward: {episode_reward}, Epsilon: {agent.epsilon}\")\n",
        "    agent.save('simple_sarsa_agent_final.pkl')\n",
        "    return rewards\n",
        "\n",
        "# Initializing the SARSA agent with adjusted hyperparameters\n",
        "sarsa_agent = SARSAAgent(state_size, action_size, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01)\n",
        "\n",
        "# Training the SARSA Agent\n",
        "sarsa_rewards = train_sarsa_agent(env, sarsa_agent, episodes=10000)\n",
        "\n",
        "# Plotting rewards\n",
        "plt.plot(sarsa_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('SARSA Agent Training Rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcAFI6-UwDn6"
      },
      "source": [
        "## MCTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zrop5ilXwEpL",
        "outputId": "3b946f18-5d18-4fef-849f-3d7254a5c47f"
      },
      "outputs": [],
      "source": [
        "def train_mcts_agent(env, agent, episodes=10000):\n",
        "    rewards = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state, env)\n",
        "            action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "            next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        print(f\"Episode: {e+1}/{episodes}, Reward: {episode_reward}\")\n",
        "\n",
        "    average_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards)\n",
        "    min_reward = np.min(rewards)\n",
        "    max_reward = np.max(rewards)\n",
        "\n",
        "    print(f\"Average Reward over {episodes} episodes: {average_reward}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward}\")\n",
        "    print(f\"Minimum Reward: {min_reward}\")\n",
        "    print(f\"Maximum Reward: {max_reward}\")\n",
        "\n",
        "    agent.save('mcts_agent.pkl')\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Initializing and train the MCTS agent\n",
        "mcts_agent = MCTSAgent(num_simulations=500)\n",
        "\n",
        "# Training MCTS Agent\n",
        "print(\"Training MCTS Agent\")\n",
        "mcts_rewards = train_mcts_agent(env, mcts_agent, episodes=10000)\n",
        "\n",
        "# Plotting rewards\n",
        "plt.plot(mcts_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('MCTS Agent Training Rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sn6eKGIdoyf"
      },
      "source": [
        "statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qkhlyK0JdoWH",
        "outputId": "e5b0401e-a9ab-4359-b8d8-592b83f4a14f"
      },
      "outputs": [],
      "source": [
        "def plot_training_statistics(rewards, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(rewards, label='Rewards')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting the training statistics for each agent\n",
        "plot_training_statistics(q_learning_rewards, \"Q-Learning Agent Training Rewards\")\n",
        "plot_training_statistics(dqn_rewards, \"DQN Agent Training Rewards\")\n",
        "plot_training_statistics(bayesian_rewards, \"Bayesian Agent Training Rewards\")\n",
        "plot_training_statistics(sarsa_rewards, \"SARSA Agent Training Rewards\")\n",
        "plot_training_statistics(mcts_rewards, \"MCTS Agent Training Rewards\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sGMAM4FM9lNT",
        "outputId": "64a6fe86-2765-492b-f611-5131db42d90b"
      },
      "outputs": [],
      "source": [
        "def plot_rolling_average(rewards, window_size=50, title='Reward Trend'):\n",
        "    rolling_avg = pd.Series(rewards).rolling(window=window_size).mean()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(rolling_avg)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_rolling_average(q_learning_rewards, title='Q-Learning Agent Reward Trend')\n",
        "plot_rolling_average(dqn_rewards, title='DQN Agent Reward Trend')\n",
        "plot_rolling_average(bayesian_rewards, title='Bayesian Agent Reward Trend')\n",
        "plot_rolling_average(sarsa_rewards, title='SARSA Agent Reward Trend')\n",
        "plot_rolling_average(mcts_rewards, title='MCTS Agent Reward Trend')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWeadQCLUb0-"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "IA4mz4oeuqRq"
      },
      "outputs": [],
      "source": [
        "class HeuristicAgent:\n",
        "    def __init__(self, num_dice, num_faces=6):\n",
        "        self.num_dice = num_dice\n",
        "        self.num_faces = num_faces\n",
        "\n",
        "    def make_bid(self, current_bid):\n",
        "        quantity, face_value = current_bid\n",
        "        if face_value < self.num_faces:\n",
        "            return (quantity, face_value + 1)\n",
        "        else:\n",
        "            return (quantity + 1, 1)\n",
        "\n",
        "    def should_challenge(self, current_bid, observed_dice):\n",
        "        total_dice = self.num_dice * 2\n",
        "        return current_bid[0] > total_dice // 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "8FYW1ElXuug1"
      },
      "outputs": [],
      "source": [
        "def preprocess_state(state):\n",
        "    return np.concatenate([\n",
        "        state[\"dice_count\"],\n",
        "        state[\"current_bid\"],\n",
        "        state[\"scores\"],\n",
        "        [state[\"current_player\"]],\n",
        "        state[\"players_dice\"].flatten()\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_action_space():\n",
        "    actions = [(0, quantity, face_value) for quantity in range(1, 11) for face_value in range(1, 7)]\n",
        "    actions.append((1, 0, 0))  # Challenge action\n",
        "    return actions\n",
        "\n",
        "\n",
        "def test_against_heuristic(env, agent, heuristic_agent, episodes=100):\n",
        "    rewards = []\n",
        "    heuristic_rewards = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        heuristic_episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            current_player = state[\"current_player\"]\n",
        "            opponent = 1 if current_player == 0 else 0\n",
        "            observed_dice = state[\"players_dice\"][opponent]\n",
        "            current_bid = state[\"current_bid\"]\n",
        "\n",
        "            if current_player == 0:\n",
        "                if isinstance(agent, BayesianAgent):\n",
        "                    if agent.should_challenge(current_bid, observed_dice):\n",
        "                        action = (1, 0, 0)  # Challenge\n",
        "                    else:\n",
        "                        quantity, face_value = agent.make_bid(current_bid, observed_dice)\n",
        "                        action = (0, quantity, face_value)\n",
        "                else:\n",
        "                    state_flat = preprocess_state(state)\n",
        "                    action_index = agent.act(state_flat)\n",
        "                    action = get_action_space()[action_index]\n",
        "            else:\n",
        "                if heuristic_agent.should_challenge(current_bid, observed_dice):\n",
        "                    action = (1, 0, 0)  # Challenge\n",
        "                else:\n",
        "                    quantity, face_value = heuristic_agent.make_bid(current_bid)\n",
        "                    action = (0, quantity, face_value)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if current_player == 0:\n",
        "                episode_reward += reward\n",
        "            else:\n",
        "                heuristic_episode_reward += reward\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        heuristic_rewards.append(heuristic_episode_reward)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_heuristic_reward = np.mean(heuristic_rewards)\n",
        "\n",
        "    print(f\"Average Reward for Agent over {episodes} episodes: {avg_reward}\")\n",
        "    print(f\"Average Reward for Heuristic Agent over {episodes} episodes: {avg_heuristic_reward}\")\n",
        "\n",
        "    return avg_reward, avg_heuristic_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wKkNxnBuxXd",
        "outputId": "25b81374-d2c9-475a-badd-9915c082cd88"
      },
      "outputs": [],
      "source": [
        "# Initialize environment and agents\n",
        "env = LiarDiceEnv()\n",
        "state_size = 17\n",
        "\n",
        "# Print the action space size\n",
        "action_space = get_action_space()\n",
        "action_size = len(action_space)\n",
        "print(f\"Action Size: {action_size}\")\n",
        "\n",
        "# Initialize agents\n",
        "q_learning_agent = QLearningAgent(state_size, action_size)\n",
        "dqn_agent = DQNAgent(state_size, action_size)\n",
        "bayesian_agent = BayesianAgent(num_dice=5)\n",
        "sarsa_agent = SARSAAgent(state_size, action_size)\n",
        "mcts_agent = MCTSAgent(num_simulations=1000)\n",
        "\n",
        "# Load trained agents\n",
        "q_learning_agent.load('q_learning_agent_final.pkl')\n",
        "dqn_agent.load('dqn_agent_final.pkl')\n",
        "bayesian_agent.load('bayesian_agent_final.pkl')\n",
        "sarsa_agent.load('simple_sarsa_agent_final.pkl')\n",
        "mcts_agent.load('mcts_agent.pkl')\n",
        "\n",
        "# Initialize heuristic agent\n",
        "heuristic_agent = HeuristicAgent(num_dice=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test agents against heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "nevpg5JUu4cE",
        "outputId": "0129a911-ba6d-4aeb-e57d-7c8d7a53fea5"
      },
      "outputs": [],
      "source": [
        "print(\"Testing Q-Learning Agent against Heuristic\")\n",
        "q_learning_avg_reward, q_learning_heuristic_avg_reward = test_against_heuristic(env, q_learning_agent, heuristic_agent, episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "593zHY49vE--",
        "outputId": "859209dc-6d20-4635-d79d-33da33ed76a3"
      },
      "outputs": [],
      "source": [
        "print(\"Testing DQN Agent against Heuristic\")\n",
        "dqn_avg_reward, dqn_heuristic_avg_reward = test_against_heuristic(env, dqn_agent, heuristic_agent, episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQzDz3y2vHiP"
      },
      "outputs": [],
      "source": [
        "print(\"Testing Bayesian Agent against Heuristic\")\n",
        "bayesian_avg_reward, bayesian_heuristic_avg_reward = test_against_heuristic(env, bayesian_agent, heuristic_agent, episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glkgbmiDvI0q"
      },
      "outputs": [],
      "source": [
        "print(\"Testing SARSA Agent against Heuristic\")\n",
        "sarsa_avg_reward, sarsa_heuristic_avg_reward = test_against_heuristic(env, sarsa_agent, heuristic_agent, episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkwCjSF0vJ5h"
      },
      "outputs": [],
      "source": [
        "print(\"Testing MCTS Agent against Heuristic\")\n",
        "mcts_avg_reward, mcts_heuristic_avg_reward = test_against_heuristic(env, mcts_agent, heuristic_agent, episodes=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edMoZ4FSu73J"
      },
      "source": [
        "## Testing 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD3FEaYhUc-8",
        "outputId": "2efd24c0-e3b6-4922-e779-13a26750cc74"
      },
      "outputs": [],
      "source": [
        "def preprocess_state(state):\n",
        "    return np.concatenate([\n",
        "        state[\"dice_count\"],\n",
        "        state[\"current_bid\"],\n",
        "        state[\"scores\"],\n",
        "        [state[\"current_player\"]],\n",
        "        state[\"players_dice\"].flatten()\n",
        "    ])\n",
        "\n",
        "def get_action_space():\n",
        "    actions = [(0, quantity, face_value) for quantity in range(1, 11) for face_value in range(1, 7)]\n",
        "    actions.append((1, 0, 0))  # Challenge action\n",
        "    return actions\n",
        "\n",
        "def test_agent(env, agent, episodes=1000, render=False):\n",
        "    rewards = []\n",
        "    action_space = get_action_space()\n",
        "    action_size = len(action_space)\n",
        "    print(f\"Testing agent with action size: {action_size}\")\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = preprocess_state(state)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            if isinstance(agent, DQNAgent):\n",
        "                action_index = agent.act(state)\n",
        "                if action_index >= action_size or action_index < 0: # Skip invalid action index\n",
        "                    print(f\"Invalid action index {action_index} for DQNAgent with action size {action_size}\")\n",
        "                    continue  \n",
        "            elif isinstance(agent, QLearningAgent) or isinstance(agent, SARSAAgent):\n",
        "                action_index = agent.get_action(state)\n",
        "                if action_index >= action_size or action_index < 0: # Skip invalid action index\n",
        "                    print(f\"Invalid action index {action_index} for agent with action size {action_size}\")\n",
        "                    continue \n",
        "            elif isinstance(agent, BayesianAgent):\n",
        "                original_state = env.reset() \n",
        "                observed_dice = original_state[\"players_dice\"][1] if original_state[\"current_player\"] == 0 else original_state[\"players_dice\"][0]\n",
        "                current_bid = original_state[\"current_bid\"]\n",
        "                if agent.should_challenge(current_bid, observed_dice):\n",
        "                    action_index = len(action_space) - 1  # Challenge action\n",
        "                else:\n",
        "                    quantity, face_value = agent.make_bid(current_bid, observed_dice)\n",
        "                    action_index = action_space.index((0, quantity, face_value))\n",
        "            elif isinstance(agent, MCTSAgent):\n",
        "                action = agent.select_action(state, env)\n",
        "                action_index = action_space.index(action)\n",
        "\n",
        "            action = action_space[action_index]\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = preprocess_state(next_state)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            if render:\n",
        "                env.render()\n",
        "        rewards.append(episode_reward)\n",
        "    return rewards\n",
        "\n",
        "# Initialize environment and agents\n",
        "env = LiarDiceEnv()\n",
        "state_size = 17\n",
        "\n",
        "# Print the action space size\n",
        "action_space = get_action_space()\n",
        "action_size = len(action_space)\n",
        "print(f\"Action Size: {action_size}\")\n",
        "\n",
        "# Initialize agents\n",
        "q_learning_agent = QLearningAgent(state_size, action_size)\n",
        "dqn_agent = DQNAgent(state_size, action_size)\n",
        "bayesian_agent = BayesianAgent(num_dice=5)\n",
        "sarsa_agent = SARSAAgent(state_size, action_size)\n",
        "mcts_agent = MCTSAgent(num_simulations=1000)\n",
        "\n",
        "# Load trained agents\n",
        "q_learning_agent.load('q_learning_agent_final.pkl')\n",
        "dqn_agent.load('dqn_agent_final.pkl')\n",
        "bayesian_agent.load('bayesian_agent_final.pkl')\n",
        "sarsa_agent.load('simple_sarsa_agent_final.pkl')\n",
        "mcts_agent.load('mcts_agent.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25bZNrNYrQcs",
        "outputId": "c6a6376b-7777-454a-80e1-05964243af55"
      },
      "outputs": [],
      "source": [
        "# Test agents\n",
        "print(\"Testing Q-Learning Agent\")\n",
        "q_learning_test_rewards = test_agent(env, q_learning_agent, episodes=100, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCPWeUvJrRqf",
        "outputId": "8050c962-ee5e-4ed9-bf37-2405efb6131e"
      },
      "outputs": [],
      "source": [
        "print(\"Testing DQN Agent\")\n",
        "dqn_test_rewards = test_agent(env, dqn_agent, episodes=100, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YHNsKcRirSsk",
        "outputId": "abc1f4cc-7ede-41e9-fae9-fa62ec960931"
      },
      "outputs": [],
      "source": [
        "print(\"Testing Bayesian Agent\")\n",
        "bayesian_test_rewards = test_agent(env, bayesian_agent, episodes=100, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GowiLVvArThS",
        "outputId": "e85b2509-422a-4bec-ebac-752476221517"
      },
      "outputs": [],
      "source": [
        "print(\"Testing SARSA Agent\")\n",
        "sarsa_test_rewards = test_agent(env, sarsa_agent, episodes=100, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "TFKkf260rv4_",
        "outputId": "01cc7887-19bc-48dd-cd64-56d9ebd0e8f7"
      },
      "outputs": [],
      "source": [
        "print(\"Testing MCTS Agent\")\n",
        "mcts_test_rewards = test_agent(env, mcts_agent, episodes=100, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "8JueUKMFrYFD",
        "outputId": "67cb9f03-4d47-458d-dee0-3c56ac02becf"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame to display the statistics\n",
        "data = {\n",
        "    \"Agent\": [\"Q-Learning\", \"DQN\", \"Bayesian\", \"SARSA\", \"MCTS\"],\n",
        "    \"Average Reward\": [np.mean(q_learning_test_rewards), np.mean(dqn_test_rewards), np.mean(bayesian_test_rewards), np.mean(sarsa_test_rewards), np.mean(mcts_test_rewards)],\n",
        "    \"Std Dev Reward\": [np.std(q_learning_test_rewards), np.std(dqn_test_rewards), np.std(bayesian_test_rewards), np.std(sarsa_test_rewards), np.std(mcts_test_rewards)],\n",
        "    \"Min Reward\": [np.min(q_learning_test_rewards), np.min(dqn_test_rewards), np.min(bayesian_test_rewards), np.min(sarsa_test_rewards), np.min(mcts_test_rewards)],\n",
        "    \"Max Reward\": [np.max(q_learning_test_rewards), np.max(dqn_test_rewards), np.max(bayesian_test_rewards), np.max(sarsa_test_rewards), np.max(mcts_test_rewards)],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"\\nAgent Performance Summary:\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8XXByFLqC7N"
      },
      "outputs": [],
      "source": [
        "# Plotting the rewards for each agent\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(data=[q_learning_rewards, dqn_rewards, bayesian_rewards, sarsa_rewards, mcts_rewards], palette=\"Set2\")\n",
        "plt.xticks(ticks=range(5), labels=[\"Q-Learning\", \"DQN\", \"Bayesian\", \"SARSA\", \"MCTS\"])\n",
        "plt.xlabel(\"Agent\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.title(\"Reward Distribution for Each Agent\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting the average rewards with error bars\n",
        "plt.figure(figsize=(14, 8))\n",
        "mean_rewards = df[\"Average Reward\"]\n",
        "std_rewards = df[\"Std Dev Reward\"]\n",
        "agents = df[\"Agent\"]\n",
        "\n",
        "plt.errorbar(agents, mean_rewards, yerr=std_rewards, fmt='o', capsize=5, capthick=2, ecolor='gray')\n",
        "plt.xlabel(\"Agent\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Average Rewards with Standard Deviation for Each Agent\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting the minimum and maximum rewards for each agent\n",
        "plt.figure(figsize=(14, 8))\n",
        "x = np.arange(len(df[\"Agent\"]))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, df[\"Min Reward\"], width, label='Min Reward')\n",
        "rects2 = ax.bar(x + width/2, df[\"Max Reward\"], width, label='Max Reward')\n",
        "\n",
        "ax.set_xlabel('Agent')\n",
        "ax.set_ylabel('Reward')\n",
        "ax.set_title('Minimum and Maximum Rewards for Each Agent')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df[\"Agent\"])\n",
        "ax.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Cumulative Reward Plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(np.cumsum(q_learning_rewards), label=\"Q-Learning\")\n",
        "plt.plot(np.cumsum(dqn_rewards), label=\"DQN\")\n",
        "plt.plot(np.cumsum(bayesian_rewards), label=\"Bayesian\")\n",
        "plt.plot(np.cumsum(sarsa_rewards), label=\"SARSA\")\n",
        "plt.plot(np.cumsum(mcts_rewards), label=\"MCTS\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Cumulative Reward\")\n",
        "plt.title(\"Cumulative Reward Over Episodes for Each Agent\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Reward Trends Plot (Rolling Average)\n",
        "plt.figure(figsize=(14, 8))\n",
        "window_size = 50\n",
        "plt.plot(pd.Series(q_learning_rewards).rolling(window_size).mean(), label=\"Q-Learning\")\n",
        "plt.plot(pd.Series(dqn_rewards).rolling(window_size).mean(), label=\"DQN\")\n",
        "plt.plot(pd.Series(bayesian_rewards).rolling(window_size).mean(), label=\"Bayesian\")\n",
        "plt.plot(pd.Series(sarsa_rewards).rolling(window_size).mean(), label=\"SARSA\")\n",
        "plt.plot(pd.Series(mcts_rewards).rolling(window_size).mean(), label=\"MCTS\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward (Rolling Average)\")\n",
        "plt.title(f\"Reward Trends Over Episodes (Window Size = {window_size})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmxhgNlUZs7"
      },
      "source": [
        "# All Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVWaF95EUbkL"
      },
      "outputs": [],
      "source": [
        "def switch_agent(agent1, agent2, current_player):\n",
        "    return agent2 if current_player == 1 else agent1\n",
        "\n",
        "def train_agents_against_each_other(env, agent1, agent2, episodes=1000):\n",
        "    rewards_agent1 = []\n",
        "    rewards_agent2 = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        current_agent = agent1 if state[\"current_player\"] == 0 else agent2\n",
        "        done = False\n",
        "        episode_reward_agent1 = 0\n",
        "        episode_reward_agent2 = 0\n",
        "\n",
        "        while not done:\n",
        "            if isinstance(current_agent, DQNAgent):\n",
        "                state_np = np.concatenate([state[\"dice_count\"], state[\"current_bid\"], state[\"scores\"], [state[\"current_player\"]]])\n",
        "                action = current_agent.act(state_np)\n",
        "                action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "                next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "                next_state_np = np.concatenate([next_state[\"dice_count\"], next_state[\"current_bid\"], next_state[\"scores\"], [next_state[\"current_player\"]]])\n",
        "                current_agent.remember(state_np, action, reward, next_state_np, done)\n",
        "                state = next_state\n",
        "            elif isinstance(current_agent, QLearningAgent):\n",
        "                state_np = tuple(state[\"dice_count\"]) + tuple(state[\"current_bid\"]) + tuple(state[\"scores\"]) + (state[\"current_player\"],)\n",
        "                action = current_agent.get_action(state_np)\n",
        "                action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "                next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "                next_state_np = tuple(next_state[\"dice_count\"]) + tuple(next_state[\"current_bid\"]) + tuple(next_state[\"scores\"]) + (next_state[\"current_player\"],)\n",
        "                current_agent.update_q_table(state_np, action, reward, next_state_np)\n",
        "                state = next_state\n",
        "            elif isinstance(current_agent, SARSAAgent):\n",
        "                state_np = tuple(state[\"dice_count\"]) + tuple(state[\"current_bid\"]) + tuple(state[\"scores\"]) + (state[\"current_player\"],)\n",
        "                action = current_agent.get_action(state_np)\n",
        "                action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "                next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "                next_state_np = tuple(next_state[\"dice_count\"]) + tuple(next_state[\"current_bid\"]) + tuple(next_state[\"scores\"]) + (next_state[\"current_player\"],)\n",
        "                next_action = current_agent.get_action(next_state_np)\n",
        "                current_agent.update_q_table(state_np, action, reward, next_state_np, next_action)\n",
        "                state = next_state\n",
        "            elif isinstance(current_agent, BayesianAgent):\n",
        "                observed_dice = state[\"players_dice\"][1] if state[\"current_player\"] == 0 else state[\"players_dice\"][0]\n",
        "                current_bid = state[\"current_bid\"]\n",
        "                if current_agent.should_challenge(current_bid, observed_dice):\n",
        "                    action = (1, 0, 0)  # Challenge\n",
        "                else:\n",
        "                    quantity, face_value = current_agent.make_bid(current_bid, observed_dice)\n",
        "                    action = (0, quantity, face_value)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                state = next_state\n",
        "            elif isinstance(current_agent, MCTSAgent):\n",
        "              action = current_agent.select_action(state, env)\n",
        "              action_type, quantity, face_value = action // 60, (action % 60) // 6, action % 6 + 1\n",
        "              next_state, reward, done, _ = env.step((action_type, quantity, face_value))\n",
        "              state = next_state\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported agent type\")\n",
        "\n",
        "            if state[\"current_player\"] == 0:\n",
        "                episode_reward_agent1 += reward\n",
        "            else:\n",
        "                episode_reward_agent2 += reward\n",
        "\n",
        "            current_agent = switch_agent(agent1, agent2, state[\"current_player\"])\n",
        "\n",
        "        rewards_agent1.append(episode_reward_agent1)\n",
        "        rewards_agent2.append(episode_reward_agent2)\n",
        "\n",
        "        # Update target model for DQN agents periodically\n",
        "        if isinstance(agent1, DQNAgent) and e % 50 == 0:\n",
        "            agent1.update_target_model()\n",
        "        if isinstance(agent2, DQNAgent) and e % 50 == 0:\n",
        "            agent2.update_target_model()\n",
        "\n",
        "        print(f\"Episode: {e+1}/{episodes}, Reward Agent 1: {episode_reward_agent1}, Reward Agent 2: {episode_reward_agent2}\")\n",
        "\n",
        "    return rewards_agent1, rewards_agent2\n",
        "\n",
        "\n",
        "# Initializing the environment and agents\n",
        "# env = LiarDiceEnv()\n",
        "# state_size = 17  # dice_count(2) + current_bid(2) + scores(2) + current_player(1)\n",
        "# action_size = 2 * 10 * 6  # 2 types, 10 quantities, 6 face values\n",
        "# q_learning_agent = QLearningAgent(state_size, action_size)\n",
        "# dqn_agent = DQNAgent(state_size, action_size)\n",
        "# bayesian_agent = BayesianAgent(num_dice=5)\n",
        "# sarsa_agent = SARSAAgent(state_size, action_size)\n",
        "# mcts_agent = MCTSAgent(state_size, action_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK7sIg4LCBli"
      },
      "outputs": [],
      "source": [
        "# Train Q-Learning Agent vs DQN Agent\n",
        "rewards_q_learning_vs_dqn = train_agents_against_each_other(env, q_learning_agent, dqn_agent, episodes=1000)\n",
        "\n",
        "# Train Q-Learning Agent vs Bayesian Agent\n",
        "rewards_q_learning_vs_bayesian = train_agents_against_each_other(env, q_learning_agent, bayesian_agent, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDXPAAOoCEvR"
      },
      "outputs": [],
      "source": [
        "# Train DQN Agent vs Bayesian Agent\n",
        "rewards_dqn_vs_bayesian = train_agents_against_each_other(env, dqn_agent, bayesian_agent, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6hwWoRaCGVK"
      },
      "outputs": [],
      "source": [
        "# Train SARSA Agent vs other agents\n",
        "rewards_sarsa_vs_dqn = train_agents_against_each_other(env, sarsa_agent, dqn_agent, episodes=1000)\n",
        "rewards_sarsa_vs_q_learning = train_agents_against_each_other(env, sarsa_agent, q_learning_agent, episodes=1000)\n",
        "rewards_sarsa_vs_bayesian = train_agents_against_each_other(env, sarsa_agent, bayesian_agent, episodes=1000)\n",
        "rewards_sarsa_vs_mcts = train_agents_against_each_other(env, sarsa_agent, mcts_agent, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F52hMps9CHXF"
      },
      "outputs": [],
      "source": [
        "# Train MCTS Agent vs other agents\n",
        "rewards_mcts_vs_dqn = train_agents_against_each_other(env, mcts_agent, dqn_agent, episodes=1000)\n",
        "rewards_mcts_vs_q_learning = train_agents_against_each_other(env, mcts_agent, q_learning_agent, episodes=1000)\n",
        "rewards_mcts_vs_bayesian = train_agents_against_each_other(env, mcts_agent, bayesian_agent, episodes=1000)\n",
        "rewards_mcts_vs_sarsa = train_agents_against_each_other(env, mcts_agent, sarsa_agent, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DimjoL_JpO64"
      },
      "outputs": [],
      "source": [
        "def plot_smoothed_training_statistics(rewards, title, window=50):\n",
        "    smoothed_rewards = pd.Series(rewards).rolling(window=window).mean()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(smoothed_rewards, label='Smoothed Rewards')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the smoothed training statistics for each matchup\n",
        "plot_smoothed_training_statistics(rewards_q_learning_vs_dqn[0], \"Q-Learning Agent vs DQN Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_q_learning_vs_dqn[1], \"DQN Agent vs Q-Learning Agent (Smoothed)\")\n",
        "\n",
        "plot_smoothed_training_statistics(rewards_q_learning_vs_bayesian[0], \"Q-Learning Agent vs Bayesian Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_q_learning_vs_bayesian[1], \"Bayesian Agent vs Q-Learning Agent (Smoothed)\")\n",
        "\n",
        "plot_smoothed_training_statistics(rewards_dqn_vs_bayesian[0], \"DQN Agent vs Bayesian Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_dqn_vs_bayesian[1], \"Bayesian Agent vs DQN Agent (Smoothed)\")\n",
        "\n",
        "# Plot the smoothed training statistics for SARSA matchups\n",
        "plot_smoothed_training_statistics(rewards_sarsa_vs_dqn[0], \"SARSA Agent vs DQN Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_sarsa_vs_dqn[1], \"DQN Agent vs SARSA Agent (Smoothed)\")\n",
        "\n",
        "plot_smoothed_training_statistics(rewards_sarsa_vs_q_learning[0], \"SARSA Agent vs Q-Learning Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_sarsa_vs_q_learning[1], \"Q-Learning Agent vs SARSA Agent (Smoothed)\")\n",
        "\n",
        "# # Plot the smoothed training statistics for MCTS matchups\n",
        "plot_smoothed_training_statistics(rewards_mcts_vs_dqn[0], \"MCTS Agent vs DQN Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_mcts_vs_dqn[1], \"DQN Agent vs MCTS Agent (Smoothed)\")\n",
        "\n",
        "plot_smoothed_training_statistics(rewards_mcts_vs_q_learning[0], \"MCTS Agent vs Q-Learning Agent (Smoothed)\")\n",
        "plot_smoothed_training_statistics(rewards_mcts_vs_q_learning[1], \"Q-Learning Agent vs MCTS Agent (Smoothed)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1t0GR8jB9tA"
      },
      "outputs": [],
      "source": [
        "# Plot the training statistics for each matchup\n",
        "def plot_training_statistics(rewards1, rewards2, title1, title2):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(rewards1, label=title1)\n",
        "    plt.plot(rewards2, label=title2)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(f'{title1} vs {title2}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training statistics for each matchup\n",
        "plot_training_statistics(rewards_q_learning_vs_dqn[0], rewards_q_learning_vs_dqn[1], \"Q-Learning Agent\", \"DQN Agent\")\n",
        "plot_training_statistics(rewards_q_learning_vs_bayesian[0], rewards_q_learning_vs_bayesian[1], \"Q-Learning Agent\", \"Bayesian Agent\")\n",
        "plot_training_statistics(rewards_dqn_vs_bayesian[0], rewards_dqn_vs_bayesian[1], \"DQN Agent\", \"Bayesian Agent\")\n",
        "plot_training_statistics(rewards_sarsa_vs_dqn[0], rewards_sarsa_vs_dqn[1], \"SARSA Agent\", \"DQN Agent\")\n",
        "plot_training_statistics(rewards_sarsa_vs_q_learning[0], rewards_sarsa_vs_q_learning[1], \"SARSA Agent\", \"Q-Learning Agent\")\n",
        "plot_training_statistics(rewards_sarsa_vs_bayesian[0], rewards_sarsa_vs_bayesian[1], \"SARSA Agent\", \"Bayesian Agent\")\n",
        "plot_training_statistics(rewards_sarsa_vs_mcts[0], rewards_sarsa_vs_mcts[1], \"SARSA Agent\", \"MCTS Agent\")\n",
        "plot_training_statistics(rewards_mcts_vs_dqn[0], rewards_mcts_vs_dqn[1], \"MCTS Agent\", \"DQN Agent\")\n",
        "plot_training_statistics(rewards_mcts_vs_q_learning[0], rewards_mcts_vs_q_learning[1], \"MCTS Agent\", \"Q-Learning Agent\")\n",
        "plot_training_statistics(rewards_mcts_vs_bayesian[0], rewards_mcts_vs_bayesian[1], \"MCTS Agent\", \"Bayesian Agent\")\n",
        "plot_training_statistics(rewards_mcts_vs_sarsa[0], rewards_mcts_vs_sarsa[1], \"MCTS Agent\", \"SARSA Agent\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
